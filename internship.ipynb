{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras import layers\n",
    "from keras.optimizers import SGD, Adam,RMSprop\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Lambda,Conv1D,  MaxPooling1D\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.regularizers import l2, l1\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import datetime\n",
    "import umap\n",
    "import scipy.sparse\n",
    "import sympy\n",
    "import sklearn.feature_extraction.text\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import itertools \n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.SubsMat import MatrixInfo as matlist\n",
    "from keras.models import load_model\n",
    "import scipy as sp\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for preprocessing peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding decoy sequences to the dataset\n",
    "def add_decoy(data):\n",
    "    df = pd.concat([data] * 2, ignore_index = True)\n",
    "    df_len = len(data)\n",
    "    true_label = [1] * df_len\n",
    "    print(len(true_label))\n",
    "    false_label = [0] * df_len\n",
    "    print(len(false_label))\n",
    "    labels = np.array(true_label + false_label)\n",
    "    df['label'] = labels\n",
    "    df.loc[df_len:, \"sequence\"] = np.random.permutation(df.loc[df_len:, \"sequence\"])\n",
    "    df = df.sample(frac=1.0).reset_index(drop=True)\n",
    "    return df\n",
    "def encoding_seqs_left(sequences):\n",
    "    left = list()\n",
    "    for i in range(sequences.shape[0]):\n",
    "        matrix = tokenizer.texts_to_matrix(sequences[i][1]).astype(datatype)[:, 1:]\n",
    "        matrix = (np.concatenate((matrix, np.zeros((read_length - len(sequences[i][1]), 20)))))\n",
    "        left.append(matrix)\n",
    "    sequences_left_matrix=np.array(left)\n",
    "    return sequences_left_matrix\n",
    "def encoding_seqs_right(sequences):\n",
    "    right = list()\n",
    "    for i in range(len(sequences[\"predicted_sequence\"])):\n",
    "        matrix = tokenizer.texts_to_matrix(sequences[\"predicted_sequence\"][i]).astype(datatype)[:, 1:]\n",
    "        matrix = (np.concatenate((matrix, np.zeros((read_length - len(sequences[\"predicted_sequence\"][i]), 20)))))\n",
    "        right.append(matrix)\n",
    "    sequences_right_matrix=np.array(right)\n",
    "    return sequences_right_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data -train,validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2076658 259193 259744\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./results_train_mod_mass.csv\")\n",
    "val = pd.read_csv(\"./results_valid_mod_mass.csv\")\n",
    "test = pd.read_csv(\"./results_test_mod_mass.csv\")\n",
    "print(len(train), len(val),(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding decoy peptides to data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = add_decoy(train)\n",
    "data_val = add_decoy(val)\n",
    "# get the subset of the dataset\n",
    "d_train = data_train.sample(frac=1.0).reset_index(drop=True)\n",
    "d_val = data_val.sample(frac=1.0).reset_index(drop=True)\n",
    "# extracting original sequence and its id from training data\n",
    "X_train_left = d_train.iloc[:, 1:3].values\n",
    "# extracting predicted deepnovo sequence and its id from training data\n",
    "X_train_right = pd.concat([d_train.iloc[:,1:2], d_train.iloc[:,3:4]], axis=1)\n",
    "# extracting labels for the training data\n",
    "y_train = d_train.iloc[:, 10].values\n",
    "# extracting original sequence and its id from vaidation data\n",
    "X_val_left = d_val.iloc[:, 1:3].values\n",
    "# extracting predicted deepnovo sequence and its id from vaidation data\n",
    "X_val_right = pd.concat([d_val.iloc[:,1:2], d_val.iloc[:,3:4]], axis=1)\n",
    "# extracting labels for the training data\n",
    "y_val = d_val.iloc[:, 10].values \n",
    "print(y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding of the peptides using kears tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "alphabet = \"ADEFGHKMNPQRSTVWYCLI\"\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(alphabet)\n",
    "datatype = 'int8'\n",
    "read_length = 40\n",
    "X_train_left_matrix = encoding_seqs_left(X_train_left)\n",
    "X_val_left_matrix = encoding_seqs_left(X_val_left)\n",
    "X_train_right_matrix = encoding_seqs_right(X_train_right)\n",
    "X_val_right_matrix = encoding_seqs_right(X_val_right)# get the subset of the dataset\n",
    "print(X_train_left_matrix.shape,X_train_right_matrix.shape,X_val_left_matrix.shape,X_val_right_matrix.shape )\n",
    "end = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining distance \n",
    "def distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "#defining contrastive loss (taken from keras examples based on Hadsell-et-al.'06)\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 0.9\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
    "\n",
    "# Compute classification accuracy.\n",
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "# getting labels from the pedictions probabilities\n",
    "def predict_class(pred_scores):\n",
    "    results = {}\n",
    "    results['predictions_test'] =[]\n",
    "    results['pred_label'] =[]\n",
    "    for value in range(len(pred_scores)):\n",
    "        if pred_scores[value] < 0.5:\n",
    "            results['predictions_test'].append(pred_scores[value])\n",
    "            results['pred_label'].append(1)\n",
    "        else:\n",
    "            results['predictions_test'].append(pred_scores[value])\n",
    "            results['pred_label'].append(0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model architecture\n",
    "input_shape = X_train_right_matrix.shape[1:]\n",
    "input_shape = X_train_right_matrix.shape[1:]\n",
    "main_input = Input(shape=input_shape)\n",
    "x = Conv1D(filters = 40, kernel_size = 3, name = 'conv_1')(main_input) \n",
    "x = Conv1D(filters = 80, kernel_size = 3,name = 'conv_2')(x) \n",
    "x = MaxPooling1D(pool_size = 2,name = 'pool_one')(x) \n",
    "x = Conv1D(filters = 100, kernel_size = 3,kernel_initializer = 'he_uniform', activation = 'relu',kernel_regularizer=l2(0.01),  name = 'conv_3')(x) \n",
    "x = Dropout(0.3)(x) \n",
    "x = Flatten(name = 'flatten')(x) \n",
    "x= Dropout(0.3)(x)\n",
    "x = Dense(128, kernel_initializer= 'he_uniform', activation = 'relu', kernel_regularizer=l2(0.01), name = 'dense_one')(x)\n",
    "x = Dense(64, kernel_initializer= 'he_uniform', activation = 'relu', kernel_regularizer=l2(0.01), name = 'dense_two')(x)\n",
    "network = Model(main_input, x)\n",
    "# network definition\n",
    "left_input = Input(shape=input_shape) #encoding for standard 20AAs, start token, stop token, Nmod,Mmod,Qmod and '/'\n",
    "right_input = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Because we re-use the same instance `network`,\n",
    "the weights of the network\n",
    "will be shared across the two branches\n",
    "'''\n",
    "feat_map_l = network(left_input)\n",
    "feat_map_r = network(right_input)\n",
    "print(network.summary())\n",
    "L1_layer = Lambda(distance, output_shape=dist_output_shape)\n",
    "L1_distance = L1_layer([feat_map_l, feat_map_r])\n",
    "siamese_model = Model(inputs=[left_input, right_input],outputs=L1_distance)\n",
    "siamese_model.summary()\n",
    "epochs=20\n",
    "lrate=0.001\n",
    "\n",
    "# compiling the model\n",
    "siamese_model.compile(loss=contrastive_loss,optimizer= Adam(lrate, decay=1.5e-4),metrics=[accuracy] )\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', mode ='max', verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_accuracy',mode='max',verbose=1)]\n",
    "history = siamese_model.fit([X_train_left_matrix, X_train_right_matrix],y_train,epochs=epochs, batch_size = 32, validation_data=([X_val_left_matrix, X_val_right_matrix], y_val), callbacks=callbacks)\n",
    "end1 = datetime.datetime.now()\n",
    "siamese_model.save_weights('./model.h5')\n",
    "plot_model(siamese_model, to_file= './final_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training loss vs Validation loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"./loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.title('Training accuracy vs Validation accuracy')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.savefig(\"./accuracy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # #  *validation model against unknown datasets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation for testing the accuracy of the model\n",
    "def check_model(data):\n",
    "    test_data = add_decoy(data)\n",
    "    X_sample_left = test_data.iloc[:, 1:3].values\n",
    "    X_sample_right = pd.concat([test_data.iloc[:,1:2], test_data.iloc[:,3:4]], axis=1)\n",
    "    y_sample = test_data.iloc[:, 10].values\n",
    "    X_sample_left_matrix = encoding_seqs_left(X_sample_left)\n",
    "    X_sample_right_matrix = encoding_seqs_right(X_sample_right)\n",
    "    model_pred = loaded_Smodel.predict([X_sample_left_matrix, X_sample_right_matrix], batch_size = 32, verbose=0)\n",
    "    accuracy = loaded_Smodel.evaluate([X_sample_left_matrix, X_sample_right_matrix], y_sample, batch_size=128)\n",
    "    print(accuracy)\n",
    "    test_data['model_prediction'] = model_pred\n",
    "    return test_data, y_sample, model_pred\n",
    "human_test_data , y_test, human_pred = check_model(test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_data = pd.read_csv(\"./results_yeast_mod_mass_new.csv\")\n",
    "yeast_test_data, y_yeast, yeast_pred = check_model(yeast_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_data = pd.read_csv(\"./test_mouse_10k.csv\")\n",
    "mouse_test_data, y_mouse, mouse_pred = check_model(mouse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance metric for the model when testes on human test data\n",
    "human_against_human_df = pd.DataFrame.from_dict(predict_class(human_pred))\n",
    "print(human_against_human_df.head()) \n",
    "\n",
    "hh_probs = human_against_human_df['predictions_test']\n",
    "\n",
    "print(type(human_against_human_df['pred_label']))\n",
    "\n",
    "y_pred_human =pd.Series(human_against_human_df['pred_label']).values\n",
    "\n",
    "print(type(y_test), type(y_pred_human))\n",
    "\n",
    "# y_test = y_test and y_pred = y_pred_human\n",
    "print(classification_report(y_test, y_pred_human))\n",
    "\n",
    "# get the auc score\n",
    "auc_human = roc_auc_score(y_test, y_pred_human)\n",
    "print(auc_human)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_human)\n",
    "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title('confusion matrix for the human test data')\n",
    "plt.colorbar()\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.xticks([0, 1])\n",
    "plt.yticks([0, 1])\n",
    "plt.grid('off')\n",
    "plt.savefig('./confusion_matrix_human')\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "             horizontalalignment='center',\n",
    "             color='white' if cm[i, j] > 0.5 else 'black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve kears  taken from(https://github.com/Tony607/ROC-Keras/blob/master/ROC-Keras.ipynb)\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_human, tpr_human, thresholds_human = roc_curve(y_test, y_pred_human)\n",
    "fpr_yeast, tpr_yeast, thresholds_yeast = roc_curve(y_yeast, y_pred_yeast)\n",
    "fpr_mouse, tpr_mouse, thresholds_mouse = roc_curve(y_mouse, y_pred_mouse)\n",
    "from sklearn.metrics import auc\n",
    "auc_human = auc(fpr_human, tpr_human)\n",
    "auc_yeast = auc(fpr_yeast, tpr_yeast)\n",
    "auc_mouse = auc(fpr_mouse, tpr_mouse)\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_human, tpr_human, label='human (area = {:.3f})'.format(auc_human))\n",
    "plt.plot(fpr_yeast, tpr_yeast, label='yeast (area = {:.3f})'.format(auc_yeast))\n",
    "plt.plot(fpr_mouse, tpr_mouse, label='mouse (area = {:.3f})'.format(auc_mouse))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('./ROC_curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to calculate the peptide mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294.69097"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aminoacid = {\n",
    "    'A': 71.03711, # 0\n",
    "           'R': 156.10111, # 1\n",
    "           'N': 114.04293, # 2\n",
    "           'D': 115.02694, # 3\n",
    "           'C': 160.03065, # 4\n",
    "           'E': 129.04259, # 5\n",
    "           'Q': 128.05858, # 6\n",
    "           'G': 57.02146, # 7\n",
    "           'H': 137.05891, # 8\n",
    "           'I': 113.08406, # 9\n",
    "           'L': 113.08406, # 10\n",
    "           'K': 128.09496, # 11\n",
    "           'M': 131.04049, # 12\n",
    "           'F': 147.06841, # 13\n",
    "           'P': 97.05276, # 14\n",
    "           'S': 87.03203, # 15\n",
    "           'T': 101.04768, # 16\n",
    "           'W': 186.07931, # 17\n",
    "           'Y': 163.06333, # 18\n",
    "           'V': 99.06841, # 19\n",
    "}\n",
    "\n",
    "N_terminus_mass = 1.00728\n",
    "C_terminus_mass = 19.0178\n",
    "\n",
    "def protein_mass(protein):\n",
    "    temp =0\n",
    "    for char in protein:\n",
    "        temp += aminoacid[char]\n",
    "    tot_mol_wt =N_terminus_mass + temp + C_terminus_mass \n",
    "    return tot_mol_wt\n",
    "\n",
    "protein_mass('DIGAIVYGFPNK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting embedding vector from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(sample):\n",
    "    X_trial_right = pd.concat([sample.iloc[:,1:2], sample.iloc[:,3:4]], axis=1)\n",
    "    X_trial_right_matrix = encoding_seqs_right(X_trial_right)\n",
    "    embedding_query_sample_subset = intermediate_layer_model.predict(X_trial_right_matrix)\n",
    "    return embedding_query_sample_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import scipy as sp\n",
    "\n",
    "# In functional api when you call the model you are reusing the architecture and its weights automatically.\n",
    "intermediate_layer_model = Model(inputs=[network.layers[0].input],outputs=network.get_layer('dense_two').output)\n",
    "intermediate_layer_model.summary()\n",
    "# dropping duplicates from the training dataset and extracting embedding vector \n",
    "train_hu = pd.read_csv(\"./results_train_mod_mass.csv\")\n",
    "train_unique = (pd.DataFrame(train_hu, columns=['scan','sequence'])).drop_duplicates('sequence')\n",
    "print(len(train_unique))\n",
    "X_left_tr = train_unique.iloc[:, :].values\n",
    "X_h_left_matrix = encoding_seqs_left(X_left_tr)\n",
    "embedding_ref_hu_subset = intermediate_layer_model.predict(X_h_left_matrix)\n",
    "# getting a subset for trial\n",
    "human_trial = data_train.iloc[:101,:]\n",
    "embedding_query_human_subset = extract_embedding(human_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding vector for mouse sample data consisting of 101 peptides\n",
    "mouse_data = pd.read_csv(\"./test_mouse_10k.csv\")\n",
    "mouse_trial = mouse_data.iloc[:101,:]\n",
    "embedding_query_mouse_subset = extract_embedding(mouse_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding vector for yeast sample data consisting of 101 peptides\n",
    "yeast_trial = yeast_data.iloc[:101,:]\n",
    "embedding_query_yeast_subset = extract_embedding(yeast_trial)\n",
    "print(len(embedding_ref_hu_subset), len(embedding_query_yeast_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for NearestNeighbors for unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# calculating NNs for query sequence\n",
    "nbrs = NearestNeighbors(n_neighbors=100, algorithm='auto').fit(embedding_ref_hu_subset)\n",
    "def nearest_neighbor(sample_dataset, ref_dataset, embedding_query_subset):\n",
    "    query_index = sample_dataset.iloc[:,3:4].shape[0]\n",
    "    query_sequence = sample_dataset.iloc[:,3:4].reset_index(drop=True)\n",
    "    pepmass = (sample_dataset.iloc[:,-2].reset_index(drop=True)) * (sample_dataset.iloc[:,-1].reset_index(drop=True))\n",
    "    query_nn = pd.DataFrame(columns=['index','scan','sequence','predicted_sequence', 'dist', 'query_peptide','ref_peptide', 'mass_diff'])\n",
    "    KNNs=[]\n",
    "    for i in range(0,query_index):\n",
    "        q_seq = query_sequence['predicted_sequence'][i]\n",
    "        query_peptide = pepmass[i]\n",
    "        distances, indices = nbrs.kneighbors(embedding_query_subset[i:i+1,:])\n",
    "        temp = set()\n",
    "        NNs=[]\n",
    "        for j in range(0,distances.size):\n",
    "            seq_row = indices.flatten()[j]\n",
    "            scan = ref_dataset.iloc[seq_row,1:2][0]\n",
    "            ref_sequence = ref_dataset.iloc[seq_row,2:3][0]\n",
    "            ref_peptide = protein_mass(ref_sequence)\n",
    "            mass_diff = abs(ref_peptide-query_peptide)          \n",
    "            dist = distances.flatten()[j] \n",
    "            if not ref_sequence in temp:\n",
    "                NNs.append( (seq_row,scan,ref_sequence, q_seq, dist,query_peptide, ref_peptide, mass_diff) )\n",
    "                NNs.sort(key=lambda tup: tup[7])\n",
    "                temp.add(ref_sequence)\n",
    "            else:\n",
    "                continue\n",
    "        neighbors = NNs[:]\n",
    "        for element in neighbors:\n",
    "            KNNs.append( (element[0], element[1], element[2], element[3], element[4], element[5], element[6], element[7]) )\n",
    "    f = lambda x,index:tuple( i[index] for i in x)\n",
    "    tup1 = f(KNNs,0)\n",
    "    tup2 = f(KNNs,1)\n",
    "    tup3 = f(KNNs,2)\n",
    "    tup4 = f(KNNs,3)\n",
    "    tup5 = f(KNNs,4)\n",
    "    tup6 = f(KNNs,5)\n",
    "    tup7 = f(KNNs,6)\n",
    "    tup8 = f(KNNs,7)\n",
    "    for x in range(len(KNNs)):\n",
    "        query_nn = query_nn.append({'index':tup1[x], 'scan': tup2[x], 'sequence':tup3[x],'predicted_sequence':tup4[x],'dist':tup5[x], 'query_peptide':tup6[x], 'ref_peptide':tup7[x], 'mass_diff':tup8[x] },ignore_index=True)\n",
    "    return query_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_nn = nearest_neighbor(yeast_trial, data_train,embedding_query_yeast_subset)\n",
    "mouse_nn = nearest_neighbor(mouse_trial, data_train,embedding_query_mouse_subset)\n",
    "yeast_nn.to_csv('./yeast_nn.csv', index=False)\n",
    "mouse_nn.to_csv('./mouse_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating NearestNeibors results with Blosum score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blosum_score(sample):\n",
    "    validation = pd.DataFrame(columns=['predicted_sequence','scan','ref_sequence','nn_dist','blosum_score'])\n",
    "    scores = []\n",
    "    for i in range(0,len(sample)):\n",
    "        query_sequence = sample['predicted_sequence'][i]\n",
    "        ref_id = sample['scan'][i]\n",
    "        ref_sequence = sample['sequence'][i]\n",
    "#         label_gt = sample['label'][i]\n",
    "        nn_dist = sample['dist'][i]\n",
    "        x,y = (query_sequence,ref_sequence)\n",
    "        score = pairwise2.align.globalds(x, y, matrix, -9,-1)\n",
    "        score_norm = score[0][2] / score[0][4]\n",
    "        scores.append( ( query_sequence,ref_id, ref_sequence, nn_dist,score_norm) )\n",
    "    f = lambda x,index:tuple( i[index] for i in x)\n",
    "    tup1 = f(scores,0)\n",
    "    tup2 = f(scores,1)\n",
    "    tup3 = f(scores,2)\n",
    "    tup4 = f(scores,3)\n",
    "    tup5= f(scores,4)\n",
    "    for x in range(len(scores)):\n",
    "        validation = validation.append({ 'predicted_sequence':tup1[x],'scan': tup2[x],'ref_sequence':tup3[x],'nn_dist':tup4[x],'blosum_score':tup5[x]},ignore_index=True)\n",
    "    return validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_left = yeast_nn.iloc[:, 1:3].values\n",
    "X_right = pd.concat([yeast_nn.iloc[:,1:2], yeast_nn.iloc[:,3:4]], axis=1)\n",
    "X_left_matrix = encoding_seqs_left(X_left)\n",
    "X_right_matrix = encoding_seqs_right(X_right)\n",
    "print(X_left_matrix.shape,X_right_matrix.shape)\n",
    "yeast_nn_blosum = blosum_score(yeast_nn)\n",
    "yeast_nn_blosum.to_csv('./yeast_nn_blosum.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_left = mouse_nn.iloc[:, 1:3].values\n",
    "X_right = pd.concat([mouse_nn.iloc[:,1:2], mouse_nn.iloc[:,3:4]], axis=1)\n",
    "X_left_matrix = encoding_seqs_left(X_left)\n",
    "X_right_matrix = encoding_seqs_right(X_right)\n",
    "print(X_left_matrix.shape,X_right_matrix.shape)\n",
    "mouse_nn_blosum = blosum_score(mouse_nn)\n",
    "mouse_nn_blosum.to_csv('./mouse_nn_blosum.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing human proteome from Uniprot and preprocessing data for database search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import gzip\n",
    "import os\n",
    "from pyteomics import fasta, parser, mass, achrom, electrochem, auxiliary\n",
    "records = list(SeqIO.parse(\"./Human.fasta\", \"fasta\"))\n",
    "with open(\"./Human.fasta\", \"r\") as handle:\n",
    "    record_iterator = SeqIO.parse(handle, \"fasta\")\n",
    "    record_list = list(record_iterator)\n",
    "    sequence_list = [str(record.seq) for record in record_list]\n",
    "    print(\"Number of protein sequences: {0:d}\".format(len(sequence_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_peptides = set()\n",
    "for sequence in sequence_list:\n",
    "    unique_peptides.update((parser.cleave(sequence=sequence,\n",
    "          rule=parser.expasy_rules['trypsin'],\n",
    "          missed_cleavages=2)))\n",
    "    peptide_list = list(unique_peptides)\n",
    "peptide_count = len(peptide_list)\n",
    "print(\"Number of peptides: {0:d}\".format(peptide_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip peptides with undetermined amino acid 'X', or 'B'\n",
    "peptide_list = [list(peptide) for peptide in peptide_list\n",
    "                if not any(x in peptide for x in ['X', 'B', 'U', 'Z'])]\n",
    "peptide_count = len(peptide_list)\n",
    "peptide_list = [\"\".join(x) for x in peptide_list]\n",
    "print(\"Number of peptides: {0:d}\".format(peptide_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each peptide, find the mass\n",
    "peptide_mass_array = np.zeros(peptide_count)\n",
    "for index, peptide in enumerate(peptide_list):\n",
    "    peptide_mass_array[index] = protein_mass(peptide)\n",
    "db_data = {peptide_list[i]: peptide_mass_array[i] for i in range(len(peptide_list))} \n",
    "type(db_data)\n",
    "df_data = pd.DataFrame(db_data.items(), columns=['sequence','pep_mass'])\n",
    "df_data['sequence'] = df_data['sequence'].str.replace(r'[IL]','I')\n",
    "df_data = df_data.drop_duplicates('sequence')\n",
    "print(len(df_data))\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to perform database search and calculate homologues based on precursor mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursor_mass_ppm = 20.0/1000000\n",
    "def database_search(query_database, ref_database):\n",
    "    df = pd.DataFrame(columns=['query_id','query_seq','ref_sequence','ref_mass', 'query_mass', 'mass_diff'])\n",
    "    hits =[]\n",
    "    for i in range(0,len(query_database)):\n",
    "        query_id = query_database['scan'][i]\n",
    "        query_sequence = query_database['predicted_sequence'][i]\n",
    "        experimental_mass = query_database['mass'][i] \n",
    "        sequence_mass = protein_mass(query_sequence) \n",
    "        precursor_mass_tolerance = experimental_mass * precursor_mass_ppm\n",
    "        for j in range(0,len(ref_database)):\n",
    "            ref_sequence = ref_database['sequence'][j]\n",
    "            pepmass = ref_database['pep_mass'][j]\n",
    "            if abs(float(pepmass) - sequence_mass) <= precursor_mass_tolerance:\n",
    "                mass_diff = abs(float(pepmass) - sequence_mass)\n",
    "                hits.append( ( query_id,query_sequence, ref_sequence,pepmass, sequence_mass,mass_diff ) )\n",
    "    f = lambda x,index:tuple( i[index] for i in x)\n",
    "    tup1 = f(hits,0)\n",
    "    tup2 = f(hits,1)\n",
    "    tup3 = f(hits,2)\n",
    "    tup4 = f(hits,3)\n",
    "    tup5 = f(hits,4)\n",
    "    tup6 = f(hits,5)\n",
    "    for i in range(len(hits)):\n",
    "        df = df.append({'query_id': tup1[i], 'query_seq':tup2[i],'ref_sequence':tup3[i],'ref_mass':tup4[i], 'query_mass':tup5[i], 'mass_diff': tup6[i]},ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_data = pd.read_csv(\"./results_yeast_mod_mass_new.csv\")\n",
    "yeast_trial = yeast_data.iloc[:101,:]\n",
    "pepmass = (yeast_trial.iloc[:,-2].reset_index(drop=True)) * (yeast_trial.iloc[:,-1].reset_index(drop=True))\n",
    "yeast_trial['mass'] = pepmass\n",
    "yeast_trial.tail()\n",
    "yeast_hits = database_search(yeast_trial, df_data)\n",
    "yeast_hits.to_csv('./yeast_hits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_data = pd.read_csv(\"./test_mouse_10k.csv\")\n",
    "mouse_trial = mouse_data.iloc[:101,:]\n",
    "pepmass = (mouse_trial.iloc[:,-2].reset_index(drop=True)) * (mouse_trial.iloc[:,-1].reset_index(drop=True))\n",
    "mouse_trial['mass'] = pepmass\n",
    "mouse_trial.tail()\n",
    "mouse_hits = database_search(mouse_trial, df_data)\n",
    "mouse_hits.to_csv(\"./mouse_hits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate database search results with blosum score to minimise false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import pairwise2\n",
    "# Import format_alignment method\n",
    "from Bio.pairwise2 import format_alignment\n",
    "from Bio.SubsMat import MatrixInfo as matlist\n",
    "matrix = matlist.blosum62\n",
    "def blosum_score(sample):\n",
    "    validation = pd.DataFrame(columns=['predicted_sequence','scan','ref_sequence','mass_diff','blosum_score'])\n",
    "    scores = []\n",
    "    for i in range(0,len(sample)):\n",
    "        query_sequence = sample['query_seq'][i]\n",
    "        ref_id = sample['query_id'][i]\n",
    "        ref_sequence = sample['ref_sequence'][i]\n",
    "#         label_gt = sample['label'][i]\n",
    "        mass_diff = sample['mass_diff'][i]\n",
    "        x,y = (query_sequence,ref_sequence)\n",
    "        score = pairwise2.align.globalds(x, y, matrix, -9,-1)\n",
    "        score_norm = score[0][2] / score[0][4]\n",
    "        scores.append( ( query_sequence,ref_id, ref_sequence, mass_diff,score_norm) )\n",
    "    f = lambda x,index:tuple( i[index] for i in x)\n",
    "    tup1 = f(scores,0)\n",
    "    tup2 = f(scores,1)\n",
    "    tup3 = f(scores,2)\n",
    "    tup4 = f(scores,3)\n",
    "    tup5= f(scores,4)\n",
    "    for x in range(len(scores)):\n",
    "        validation = validation.append({ 'predicted_sequence':tup1[x],'scan': tup2[x],'ref_sequence':tup3[x],'mass_diff':tup4[x],'blosum_score':tup5[x]},ignore_index=True)\n",
    "    return validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_db_blosum = blosum_score(yeast_hits)\n",
    "yeast_db_blosum.to_csv('./yeast_db_blosum.csv', index=False)\n",
    "mouse_db_blosum = blosum_score(mouse_hits)\n",
    "mouse_db_blosum.to_csv('./mouse_db_blosum.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics for model performance in predicting homologues using cross-species proteome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_db = pd.read_csv(\"./mouse_db_blosum.csv\")\n",
    "mouse_db_filtered= mouse_db.loc[mouse_db[\"blosum_score\"] >=1]\n",
    "mouse_hit_count = mouse_db_filtered.groupby([\"predicted_sequence\"]).size().reset_index(name='database hits')\n",
    "mouse_hit_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_nn=pd.read_csv(\"./mouse_nn_blosum.csv\")\n",
    "mouse_nn_filtered=mouse_nn.loc[mouse_nn[\"blosum_score\"]>=1]\n",
    "mouse_hits= mouse_nn_filtered.groupby([\"predicted_sequence\"]).size().reset_index(name='KNN hits')\n",
    "print(mouse_hits)\n",
    "merged_inner_mouse = pd.merge(left=mouse_hit_count, right=mouse_hits,  left_on='predicted_sequence', right_on='predicted_sequence')\n",
    "merged_mouse = merged_inner_mouse.sort_values(\"database hits\", ascending=False)\n",
    "print(merged_mouse)\n",
    "merged_mouse.plot(x=\"predicted_sequence\",y=[\"database hits\", \"KNN hits\"],  kind=\"barh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeast_db = pd.read_csv(\"./yeast_db_blosum.csv\")\n",
    "yeast_db_filtered= yeast_db.loc[yeast_db[\"blosum_score\"] >=1]\n",
    "yeast_hit_count = yeast_db_filtered.groupby([\"predicted_sequence\"]).size().reset_index(name='database hits')\n",
    "print(yeast_hit_count)\n",
    "yeast_nn=pd.read_csv(\"./yeast_nn_blosum.csv\")\n",
    "yeast_nn_filtered=yeast_nn.loc[yeast_nn[\"blosum_score\"] >=1]\n",
    "yeast_hits = yeast_nn_filtered.groupby([\"predicted_sequence\"]).size().reset_index(name='KNN hits')\n",
    "print(yeast_hits)\n",
    "merged_inner_yeast = pd.merge(left=yeast_hit_count, right=yeast_hits, left_on='predicted_sequence', right_on='predicted_sequence')\n",
    "merged_yeast = merged_inner_yeast.sort_values(\"database hits\", ascending=False)\n",
    "print(merged_yeast)\n",
    "merged_yeast.plot(x=\"predicted_sequence\",y=[\"database hits\", \"KNN hits\"],  kind=\"barh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
